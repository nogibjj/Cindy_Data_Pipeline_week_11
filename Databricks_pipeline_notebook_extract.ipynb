{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89eb05a8-8463-4d87-97fb-8df3db864cdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded to /tmp/grad-students.csv\nData loaded into Spark DataFrame with schema from /tmp/grad-students.csv\nRaw data saved as Delta table: ids706_data_engineering.default.grad_students_raw\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType, \n",
    "    StructField, \n",
    "    StringType, \n",
    "    IntegerType, \n",
    "    FloatType\n",
    ")\n",
    "import requests\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Delta Lake Example\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"io.delta:delta-core_2.12:2.4.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \n",
    "            \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \n",
    "            \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"Major_code\", StringType(), True),\n",
    "    StructField(\"Major\", StringType(), True),\n",
    "    StructField(\"Major_category\", StringType(), True),\n",
    "    StructField(\"Grad_total\", IntegerType(), True),\n",
    "    StructField(\"Grad_sample_size\", IntegerType(), True),\n",
    "    StructField(\"Grad_employed\", IntegerType(), True),\n",
    "    StructField(\"Grad_full_time_year_round\", IntegerType(), True),\n",
    "    StructField(\"Grad_unemployed\", IntegerType(), True),\n",
    "    StructField(\"Grad_unemployment_rate\", FloatType(), True),\n",
    "    StructField(\"Grad_median\", IntegerType(), True),\n",
    "    StructField(\"Grad_P25\", IntegerType(), True),\n",
    "    StructField(\"Grad_P75\", IntegerType(), True),\n",
    "    StructField(\"Nongrad_total\", IntegerType(), True),\n",
    "    StructField(\"Nongrad_employed\", IntegerType(), True),\n",
    "    StructField(\"Nongrad_full_time_year_round\", IntegerType(), True),\n",
    "    StructField(\"Nongrad_unemployed\", IntegerType(), True),\n",
    "    StructField(\"Nongrad_unemployment_rate\", FloatType(), True),\n",
    "    StructField(\"Nongrad_median\", IntegerType(), True),\n",
    "    StructField(\"Nongrad_P25\", IntegerType(), True),\n",
    "    StructField(\"Nongrad_P75\", IntegerType(), True),\n",
    "    StructField(\"Grad_share\", FloatType(), True),\n",
    "    StructField(\"Grad_premium\", FloatType(), True)\n",
    "])\n",
    "\n",
    "def extract_spark() -> None:\n",
    "    url = \"https://raw.githubusercontent.com/fivethirtyeight/data/master/college-majors/grad-students.csv\"\n",
    "    file_path = \"/tmp/grad-students.csv\"\n",
    "\n",
    "    # Download CSV file\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        with open(file_path, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"File downloaded to {file_path}\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to download file: {e}\")\n",
    "        return\n",
    "\n",
    "    # Load data into Spark DataFrame\n",
    "    try:\n",
    "        df = spark.read.option(\"header\", \"true\").schema(schema).csv(file_path)\n",
    "        print(\"Data loaded into Spark DataFrame\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load CSV into DataFrame: {e}\")\n",
    "        return\n",
    "\n",
    "    # Save data as Parquet (to bypass Delta issues)\n",
    "    try:\n",
    "        parquet_path = \"/tmp/parquet/grad_students_raw\"\n",
    "        df.write.format(\"parquet\").mode(\"overwrite\").save(parquet_path)\n",
    "        print(f\"Data saved as Parquet at {parquet_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save Parquet table: {e}\")\n",
    "\n",
    "# Execute extraction\n",
    "extract_spark()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Databricks_pipeline_notebook_extract",
   "widgets": {
    "task": {
     "currentValue": "",
     "nuid": "ae529953-4e1c-4175-a272-74211ac0e4f2",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "task",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "task",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
